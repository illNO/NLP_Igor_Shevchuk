{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4VbEC+MlqoKUQ9Fp0Cdkz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/illNO/NLP_Igor_Shevchuk/blob/main/NLP_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3UIv4CzJhj1",
        "outputId": "6198d8ac-74ae-4f52-cee0-b11ac4382b08"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.7)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.7)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad3IKgzJY0KX",
        "outputId": "99742a4c-e38c-47db-9496-5511e8389f18"
      },
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, SpatialDropout1D, LSTM\n",
        "from tensorflow.keras.layers import Bidirectional \n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D \n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "from sklearn.metrics import roc_auc_score \n",
        "from datasets import load_dataset\n",
        "from tensorflow.keras.datasets import imdb\n",
        "STOP_WORDS = [\"went\", \"so\", \"the\", \"and\", \"a\", \"of\", \"from\", \"to\", \"not\", \"is\", \"in\", \"that\", \"this\", \"was\", \"as\", \"with\", \"for\", \"you\", \"are\", \"it\"]\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MapaOyPfI7_z"
      },
      "source": [
        "# output directory name:\n",
        "output_dir = 'model_output/LSTM/'\n",
        "\n",
        "# training:\n",
        "epochs = 4\n",
        "batch_size = 128\n",
        "\n",
        "# vector-space embedding: \n",
        "n_dim = 64 \n",
        "max_review_length = 200 \n",
        "pad_type = trunc_type = 'pre'\n",
        "drop_embed = 0.2 \n",
        "\n",
        "# convolutional layer architecture:\n",
        "n_conv = 64  \n",
        "k_conv = 3 \n",
        "mp_size = 4\n",
        "\n",
        "# LSTM layer architecture:\n",
        "n_lstm = 64 \n",
        "drop_lstm = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir_DZAj7a8NL",
        "outputId": "f0d5b6ba-c6f9-4669-e1d1-71b555e715dd"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_last(corpus):\n",
        "  all_words = []\n",
        "  for sent in corpus:\n",
        "      tokenize_word = word_tokenize(sent)\n",
        "      for word in tokenize_word:\n",
        "          all_words.append(word)\n",
        "  return all_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av4cBeFqdLfo"
      },
      "source": [
        "all_words = tokenize_last(dataset['train']['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWlOCb54dB8R",
        "outputId": "d3664883-f301-4034-db37-e49ddedd8259"
      },
      "source": [
        "unique_words = set(all_words)\n",
        "print(len(unique_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "124836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmc104p6dB-q"
      },
      "source": [
        "vocab_length = 5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xor2v-L3dCA1"
      },
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "embedded_sentences_train = [one_hot(sent, vocab_length) for sent in dataset['train']['text']]\n",
        "embedded_sentences_test = [one_hot(sent, vocab_length) for sent in dataset['test']['text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZidr34PdCD1"
      },
      "source": [
        "x_train = pad_sequences(embedded_sentences_train, 200, padding='post')\n",
        "x_valid = pad_sequences(embedded_sentences_test, 200, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUgfyFr6dihO"
      },
      "source": [
        "y_train = np.array(dataset['train']['label'])\n",
        "y_valid = np.array(dataset['test']['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt17FfsbLTey",
        "outputId": "f21cb725-9741-4c70-dc57-98943b8752d2"
      },
      "source": [
        "n_dim = 200\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(50000, n_dim, input_length=max_review_length)) \n",
        "model.add(SpatialDropout1D(drop_embed))\n",
        "model.add(Conv1D(n_conv, k_conv, activation='relu'))\n",
        "model.add(MaxPooling1D(mp_size))\n",
        "model.add(Bidirectional(LSTM(n_lstm, dropout=drop_lstm)))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, 200, 200)          10000000  \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_10 (Spatia (None, 200, 200)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 198, 64)           38464     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_10 (MaxPooling (None, 49, 64)            0         \n",
            "_________________________________________________________________\n",
            "bidirectional_11 (Bidirectio (None, 128)               66048     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 10,105,028\n",
            "Trainable params: 10,105,028\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YphuhI2ALTjj"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "710SGTuqVU6F"
      },
      "source": [
        "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaHJ57IXg6ey",
        "outputId": "25025706-ed54-4858-eb9f-6654c6d0ec25"
      },
      "source": [
        "x_valid.shape, y_valid.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7600, 200), (7600,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX8pkv1XhJuu",
        "outputId": "5af53542-408c-4883-f602-7dc1fb24b347"
      },
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((120000, 200), (120000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcySG-X5MPp9",
        "outputId": "6802f8ea-dd44-4dbd-ddf5-7ea18f4404e7"
      },
      "source": [
        "# Batch size 128 returns error ( out of memory)\n",
        "num_epochs = 4\n",
        "model.fit(x_train, y_train, batch_size=16, epochs=num_epochs, validation_data=(x_valid, y_valid), verbose=1, callbacks=[modelcheckpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "7500/7500 [==============================] - 535s 71ms/step - loss: 0.2821 - accuracy: 0.9033 - val_loss: 0.3114 - val_accuracy: 0.8950\n",
            "Epoch 2/4\n",
            "7500/7500 [==============================] - 533s 71ms/step - loss: 0.2381 - accuracy: 0.9181 - val_loss: 0.3019 - val_accuracy: 0.8974\n",
            "Epoch 3/4\n",
            "7500/7500 [==============================] - 530s 71ms/step - loss: 0.2062 - accuracy: 0.9286 - val_loss: 0.3047 - val_accuracy: 0.8989\n",
            "Epoch 4/4\n",
            "7500/7500 [==============================] - 530s 71ms/step - loss: 0.1821 - accuracy: 0.9368 - val_loss: 0.3038 - val_accuracy: 0.8991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb944b67390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO9FPzIgMPsu",
        "outputId": "9f0016d9-75e0-4876-ecf6-21711e70d126"
      },
      "source": [
        "y_hat = model.predict_proba(x_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xCLp9CtMPxb"
      },
      "source": [
        "\"{:0.2f}\".format(roc_auc_score(y_valid, y_hat)*100.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnE28RyrMP0F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBd7nFuOMP2F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2O00X-THipr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}